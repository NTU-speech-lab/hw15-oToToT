{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "from queue import Queue \n",
    "import threading\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ototot/anaconda3/envs/ML/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hid = torch.tanh(self.fc1(state))\n",
    "        hid = torch.tanh(self.fc2(hid))\n",
    "        return F.softmax(self.fc3(hid), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorAgent():\n",
    "\n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)\n",
    "\n",
    "    def learn(self, log_probs, rewards):\n",
    "        loss = (-log_probs * rewards).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def sample(self, state):\n",
    "        action_prob = self.network(torch.FloatTensor(state).to(DEVICE))\n",
    "        action_dist = Categorical(action_prob)\n",
    "        action = action_dist.sample()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor().to(DEVICE)\n",
    "actor_agent = ActorAgent(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticAgent():\n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "\n",
    "    def learn(self, advantage):\n",
    "        loss = (advantage ** 2).sum()\n",
    "        \n",
    "#         print('%.4f' % loss.detach().item())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def evaulate(self, state):\n",
    "        return self.network(torch.FloatTensor(state).to(DEVICE))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic().to(DEVICE)\n",
    "critic_agent = CriticAgent(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_PER_BATCH = 128 # 每蒐集 128 個 episodes 更新一次 agent\n",
    "NUM_BATCH = 500         # 總共更新 500 次\n",
    "GAMMA     = 0.9         # 衰敗率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efa23f5b79047898d71ed9da16c004a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 訓練前先確認network處於train的狀態\n",
    "actor_agent.network.train()\n",
    "critic_agent.network.train()\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "prg_bar.set_description(f\"Total: ?, Final: ?\")\n",
    "\n",
    "for batch in prg_bar:\n",
    "\n",
    "    advantages, log_probs = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "    \n",
    "    # 蒐集訓練資料\n",
    "    def f(result_queue):        \n",
    "        env = gym.make('LunarLander-v2')\n",
    "        state = env.reset()\n",
    "\n",
    "        total_reward  = 0\n",
    "        local_log_probs, local_advantages = [], []\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            action, log_prob = actor_agent.sample(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            local_log_probs.append(log_prob)\n",
    "            local_advantages.append(reward + GAMMA * critic_agent.evaulate(next_state) - critic_agent.evaulate(state))\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                result_queue.put((local_log_probs, local_advantages, reward, total_reward))\n",
    "                del env\n",
    "                break\n",
    "    \n",
    "    result_queue = Queue()\n",
    "    threads = []\n",
    "    for i in range(EPISODE_PER_BATCH):\n",
    "        threads.append(threading.Thread(target=f, args=(result_queue,)))\n",
    "        threads[-1].start()\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "        local_log_probs, local_advantages, reward, total_reward = result_queue.get()\n",
    "        \n",
    "        final_rewards.append(reward)\n",
    "        total_rewards.append(total_reward)\n",
    "        advantages += local_advantages\n",
    "        log_probs += local_log_probs\n",
    "            \n",
    "\n",
    "    # 紀錄訓練過程\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # 更新網路\n",
    "    if batch % 2 == 0:\n",
    "        critic_agent.learn(torch.stack(advantages).to(DEVICE))\n",
    "    else:\n",
    "        actor_agent.learn(torch.stack(log_probs).to(DEVICE), torch.stack(advantages).detach().to(DEVICE))\n",
    "    \n",
    "    del advantages, log_probs, total_rewards, final_rewards, result_queue, threads\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_total_rewards)\n",
    "plt.title(\"Total Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_final_rewards)\n",
    "plt.title(\"Final Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_agent.network.eval()  # 測試前先將 network 切換為 evaluation 模式\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = actor_agent.sample(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_reward, reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
